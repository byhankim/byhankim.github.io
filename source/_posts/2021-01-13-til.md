---
title: 21.01.13 TIL
description:
categories:
  - TIL
tags:
  - MF
toc: true
date: 2021-01-13 00:00:00
---

# 오늘 배운 거

- 추천시스템의 두 갈래
  - 내용기반 필터링(CBF, `Content Based Filtering`)
  - 협업 필터링(CF, `Collaborative Filtering`)
    - kNN(`K-Nearest Neighbors`)
      - 아이템 기반
      - 사용자 기반
    - 행렬 분해(MF, `Matrix Factorization`): 넷플릭스 대회 우승 알고리즘
  - 이중 행렬 분해 알고리즘에 대한 빌드업 수학과 간단한 이론을 훑어보았다.
- 머신러닝, 딥러닝의 가장 밑바닥에서 이루어지는 기본적인 계산 원리 두 가지
  1. 선형회귀
     - 특정한 숫자를 예측하는 문제
     - `y=ax+b`꼴에서 `x`는 독립변수, `y`는 종속변수이다. x가 변함에 따라 y가 변하고 해당 식은 결국 하나의 모델이 되는 것이다.
     - 모델을 정하는 것 = 가중치인 `a`, `b`값을 확정지으면 된다.
     - 단순 선형 회귀: x값이 하나밖에 없는 경우
       - `최소제곱법(method of least squares)` 공식을 통해 일차함수의 기울기 a와 편향값 b(y절편)구할 수 있다.
     - 다중 선형 회귀: x가 여러개인 경우(요소의 수)
       - 다항함수이므로 `경사하강법`을 적용한다.
  2. 로지스틱 회귀
     - 0과 1, 혹은 0~n까지 이산적인 숫자로 분류하는 문제
     - loss function으로 `binary crossentropy`와 `categorical crossentropy`가 있다.
- 오차값을 계산하기 위한 오차함수(Loss Function)의 종류
  - 회귀문제
    - MSE
    - RMSE
    - MAE
    - ...
  - 분류문제
    - 이항 교차엔트로피(binary crossentropy) : 이진 클래스 분류
    - 범주형 교차엔트로피(categorical crossentropy) : 다중 클래스 분류
- MSE(Mean Squared Error)
  - 여러 변수에도 적용 가능하다.
  - 모든 오차를 더하면 음수의 경우 상쇄될 수 있으므로 각 오차를 제곱한다음 더한다.
  - 이러한 오차를 줄이기 위한 기울기와 bias값을 조금씩 변경시켜가며 최적의 a, b(가중치)를 구하는 과정이 `경사하강법`이다.
- 최적화(Optimization) 알고리즘 - `경사하강법(Gradient Descent)`
  - 다회에 걸쳐 방향성을 지니고 오차가 가장 적은 지점의 최소값을 찾아가는 방법
  - 오차함수를 그래프로 나타내게 되면 convex한 그래프가 된다. 여기서 학습률(learning rate)만큼 최저점을 향해 그래프를 타고 내려가는것을 반복하여 최소값을 찾는다.
  - SGD(Stochastic Gradient Descent): 확률적 경사하강법을 포함해 지속적으로 발전하다 Adam을 주로 사용하게 되었다.
  - Matrix Factorization 이론에 경사하강법이 적용된다.
- Matrix Factorization
  - 잠재 요인 기반 분석이다. 간단한 선형대수 개념인 내적을 통해 수많은 아이템 중 몇 가지에 대한 사용자의 평점(예시)을 기반으로 사용자의 취향을 여러 요인(factor)로 분해하고, 해당 아이템 또한 여러 factor로 분해하여 둘을 내적하면 희소행렬의 빈칸이 채워지는 구조이다.
  - 여기서 다수의 factor들의 최적화된 값을 찾는 과정에 `경사하강법`이 쓰인다.

# 느낀점

- 수학의 벽을 넘고싶다...간단한 수식을 봐도 눈이 돌아간다.
- 자잘한거라도, 튜토리얼 코드 따라치는 거라도 좋으니 뭐라도 코딩을 해야될 것 같다. 코딩력과 개발력이 쑥쑥 떨어지고있다.
- 만약 과거로 돌아갈 수만 있다면 다 필요없고 건강관리, 체력관리만 하고싶다. 점점 병원과 가까워지고 있음을 느낀다.
