---
title: 21.01.12 TIL
description:
categories:
  - TIL
tags:
  - DL, ML
---

# 오늘 배운 거
- 깃 사용법 및 강사님들의 공부방법
    - 리드미 신경쓰고 블로그 꾸준히 기록 남길것(지금 안하면 막바지가서 절대 못함)
    - 블로그/튜토리얼 등 여러 코드를 그대로 따라해보고 이해 안 되는 부분을 공부하여 내가 직접 만들어보기
    - 헤딩해보고 좌절하기
    - 해결해보기
- 머신러닝/딥러닝 전반적인 개론
    - 머신러닝: 모델을 만들어 예측한다
    - 딥러닝: 인풋에 대한 가중치를 업데이트한다
    - 머신러닝과 딥러닝의 역사
    - 딥러닝의 2대 암흑기
        - XOR문제: 다층 퍼셉트론으로 해결
        - Vanishing Gradient
            - 시그모이드를 레이어별로 쌓고 여러 층을 지나다보면 기울기 값이 소실된다
            - 제프리 힌튼: 오차역전파법
        - 오버피팅 문제: Dropout으로 과적합을 방지하기 위해 랜덤하게 신경망 노드를 끊는다. 그럼 특정 노드에 의존하지 않는 `일반화 능력`이 향상된다
    - '12 FeiFei Li의 ILSVRC 대회에서 AlexNet이 10프로의 성능 향상을 이루며 ML->DL이 각광받기 시작
- 경사하강법 빌드업을 위한 초스피드 수학
    - ML/DL/추천시스템을 막론하고 가장 중요한것은 오차를 줄이는 것이다
    - 미분(경사하강법)이 필요한 이유
        - 미분 == 접선의 기울기
        - 오차를 제곱하면 이차함수가 되고, 그 최소값을 찾는 것을 목표로 하는 과정에서 오차함수의 기울기를 타고 내려가면 기울기가 0인 지점을 찾을 수 있다. 이 과정에서 미분이 들어가는 것이다.
    - 지수, 로그: 활성화 함수인 시그모이드에 필요
    - (추후 리뷰 필요)편미분: 여러 변수 중 한 가지 변수만 미분하고 나머지는 상수로 취급한다. 이를 통해 각 변수(노드)별 기울기를 계산하고 오차역전파법을 사용하여 가중치를 업데이트할 수 있다.

# 느낀점
- 부캠도 그렇고 포스코 교육도 그렇고 기본적으로 (다변량)미적분, 확률론, 통계이론, 선형대수( + 이산수학)에 대한 선수지식을 요구한다. 죽을때까지 다 할수 있을진 모르겠지만 워낙 오래 수포자였던 탓에 찬찬히 길을 다져봐야겠다.
- 강사 두분이 각자 다른 스타일로 공부에 대한 열정과 실력을 갖추셨다. 나도 가만있으면 안될 것 같다. 같다에서 끝내지 말고 확실하게 실행에 옮기는 사람이 되고싶다.